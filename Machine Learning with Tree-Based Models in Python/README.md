# Descrição do Curso:
Árvores de decisão são modelos de **aprendizado supervisionado** usados tanto para problemas de **classificação** e quanto **regressão**.
Modelos de árvore apresentam uma **alta flexibilidade** que vem a um custo: por um lado, as árvores conseguem capturar
relações complexas não lineares; por outro lado, estão propensas a memorizar o ruído presente em um conjunto de dados.
Ao agregar as previsões de árvores treinadas de maneira diferente, os métodos de conjunto aproveitam a flexibilidade das
árvores, reduzindo a tendência de memorizar ruído. Métodos de conjunto são usados em uma variedade de campos e têm um
histórico comprovado de sucesso em muitas competições de aprendizado de máquina.

Neste curso, será apresentado o treinamento de árvores de decisão e modelos baseados em árvores com a biblioteca de aprendizado de máquina **scikit-learn**. Você entenderá as vantagens e limitações das árvores e demonstrará como o **ensemblamento** pode aliviar essas limitações, tudo enquanto pratica em conjuntos de dados do mundo real. Finalmente, você também entenderá como **ajustar os hiperparâmetros** mais influentes para obter o melhor desempenho de seus modelos.

## Conteúdo Programático:
1. Arvores de Regressão e Classificação:
Árvores de Classificação e Regressão (CART) são um conjunto de modelos de aprendizado supervisionado utilizados para problemas envolvendo classificação e regressão. Neste capítulo, você será introduzido ao algoritmo CART.

2. A Compensação Entre Viés e Variância:
A compensação entre viés e variância é um dos conceitos fundamentais em aprendizado de máquina supervisionado. Neste capítulo, você entenderá como diagnosticar os problemas de sobreajuste (overfitting) e subajuste (underfitting). Você também será apresentado ao conceito de ensemblagem, onde as previsões de vários modelos são agregadas para produzir previsões mais robustas.

3. Bagging e Florestas Aleatórias:
Bagging é um método de ensemble que envolve treinar o mesmo algoritmo muitas vezes usando diferentes subconjuntos amostrados dos dados de treinamento. Neste capítulo, você entenderá como o bagging pode ser usado para criar um conjunto de árvores. Você também aprenderá como o algoritmo de florestas aleatórias pode levar a uma maior diversidade no ensemble através da aleatorização no nível de cada divisão nas árvores que formam o ensemble.

4. Boosting:
Boosting refere-se a um método de ensemble no qual vários modelos são treinados sequencialmente, com cada modelo aprendendo com os erros de seus predecessores. Neste capítulo, você será apresentado aos dois métodos de boosting: AdaBoost e Gradient Boosting.

5. Tunagem de Modelos:
Os hiperparâmetros de um modelo de aprendizado de máquina são parâmetros que não são aprendidos a partir dos dados. Eles devem ser definidos antes de ajustar o modelo ao conjunto de treinamento. Neste capítulo, você aprenderá como ajustar os hiperparâmetros de um modelo baseado em árvore usando pesquisa em grade e validação cruzada.

Plataforma: Datacamp
