{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b2342ff-f0cb-401c-8cb9-9c1117623b67",
   "metadata": {},
   "source": [
    "# Apresentação:\n",
    "\n",
    "O objetivo desse código é dar prosseguimento no estudo de **Classificadores Lineares**, agora tratando sobre **Navie Bayes** que é um tipo de classificador baseado em **Probabilidade Condicional**.\n",
    "\n",
    "## Material de Estudo:\n",
    "\n",
    "O estudo aqui será pautado na documentação das bibliotecas utilizadas, tanto o [sklearn](https://scikit-learn.org/stable/api/sklearn.naive_bayes.html) quanto o [tensorflow](https://www.tensorflow.org/probability/overview?hl=pt-br), e será complementado por livros e notas de aula de disciplinas universitárias.\n",
    "\n",
    "* **The Elements of Statistical Learning** de _Trevor Hastie_, _Robert Tibshirani_ , _Jerome Friedman_;\n",
    "* **Probabilistic Machine Learning: An Introduction** de _Kevin P. Murphy_;\n",
    "* **Notas de Aula do Curso de Machine Learning** - Curso de Verão **IMPA**.\n",
    "\n",
    "## Abordagens Bibliográficas:\n",
    "\n",
    "O livro **Probabilistic Machine Learning: An Introduction** apresenta o _Naive Bayes_ como um tipo de caso do _Linear Discriminant Analisys_, introduzino o **LDA** como uma **classe de modelos** e não como um modelo em si. Uma classe de **modelos probabilisticos de classificação** que são baseados na **Teoria de Bayes**, ou seja, se dão na forma de,\n",
    "\n",
    "$$p(y=a|\\mathbf{x},\\boldsymbol{\\theta})=\\frac{p(\\mathbf{x}|y=a,\\boldsymbol{\\theta})p(y=a|\\boldsymbol{\\theta})}{\\sum_{b}p(\\mathbf{x}|y=b, \\boldsymbol{\\theta}p(y=b|\\boldsymbol{\\theta})}$$\n",
    "\n",
    "onde $a \\neq b,\\ \\forall{a,b \\in C}$, sendo $b$ **todas as classes possíveis** e $C$ nosso conjunto de classes, lembrando que na classificação $C \\subseteq \\mathbb{Z}$.\n",
    "\n",
    "Já o **The Elements of Statistical Learning** apresenta o _Naive Bayes_ dentro do tópico de **Métodos de Suavização de Kernel**, _Kernel Smooth Methods_ que não tem nada a ver com _kernel methods_, métodos de manipulação da dimensão de espaços vetoriais de alta dimensão (como nos algoritmos de _Support Vector Machines_ - **SVM**), mas sim são usados para suavizar dados e estimar e classificar densidades sem assumir uma forma paramétrica pré-definida para essas distribuições."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9d4828e-58a4-4e88-82e3-a5714b23f436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando datasets:\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Bibliotecas Auxiliares:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Bibliotecas para visualização:\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4e9d375-1391-4090-a7ad-55f2a43c17e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4) (150,)\n"
     ]
    }
   ],
   "source": [
    "# Instanciando dados:\n",
    "X, y = load_iris(return_X_y=True);\n",
    "print(X.shape,y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba38bb32-5d44-45f6-94e3-f2f1dd6a9e86",
   "metadata": {},
   "source": [
    "# Navie Bayes:\n",
    "\n",
    "O **Naive Bayes** é uma técnica de classificação _multclasses_ útil quando temos muitas $p$ previsoras no nosso modelo. Todavia, a matemática do modelo pressupõe **independência condicional** em cada par de características dadas as valor da variável de classe. \n",
    "\n",
    "No entanto, mesmo que a suposição de **Naive Bayes** não seja verdadeira, ela funciona razoavelmente bem. Uma razão para isso é que o modelo é bastante simples tendo complexidade da ordem de $O(CD)$ parâmetros, para $C$ classes e $D$ características, e, por isso, é relativamente **imune** ao **overfitting**.\n",
    "\n",
    "A **suposição ingênua de Bayes** corresponde ao uso de uma **densidade condicional** de classe da seguinte forma:\n",
    "\n",
    "$$p(\\mathbf{x}|y=c, \\mathbf{\\theta})=\\prod_{d=1}^Dp(x_d|y=c,\\mathbf{\\theta}_{dc})$$\n",
    "\n",
    "Onde $\\theta_{dc}$ são os parâmetros para a densidade condicional para a classe $c$ e a feature $d$. Portanto os rótulos posteriores sobre a classe são dados por,\n",
    "\n",
    "$$p(y=c|\\mathbf{x},\\mathbf{\\theta})=\\frac{p(y=c|\\boldsymbol{\\pi}\\prod_{d=1}^Dp(x_d|y=c,\\boldsymbol{\\theta_{dc}})}{\\sum_c'p(y=c'|\\boldsymbol{\\pi}\\prod_{d=1}^Dp(x_d|y=c',\\boldsymbol{\\theta}_{dc'})}$$\n",
    "\n",
    "onde $\\pi_c$ é a probabilidade a priori da classe $c$ e $\\boldsymbol{\\theta}=(\\boldsymbol{\\pi},\\{\\boldsymbol{\\theta}_{dc}\\})$ são todos os parametros. Assim então definimos o Classificador **Naive Bayes** ou **NBC**.\n",
    "\n",
    "* No caso $Normal$, $\\boldsymbol{\\Sigma}_k$ é **diagonal**, então,\n",
    "    * $$\\delta_k(x) \\propto \\log\\left( \\pi_k \\prod_{j=1}^{p} f_{kj}(x_j) \\right) = \\log \\pi_k - \\frac{1}{2}\\sum_{j=1}^p\\left[\\frac{(x_j - \\mu_{kj})^2}{\\sigma_{kj}^2}+\\log{\\sigma_{kj}^2}\\right]\r\n",
    "$$\n",
    "\n",
    "* **Naive Bayes** pode ser usado também para **variáveis quantitativas**: basta usar uma estimativa da \n",
    "função de probabilidade (e.g., histogramas.\n",
    "*  Ele é um método considerado **generativos**, porque, diferente de métodos  **discriminativos** como **Regressão Logistica** e **Regressão Multinomial**, que estimam $p[Y|X]$, eles estimam $p[X|Y]$.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ed61b5-1872-4411-be3f-487fd7d2830e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
